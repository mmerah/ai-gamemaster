# models.yaml - Configuration for Llama.cpp server instances

# Model Configurations
# Use descriptive keys for easy selection via command line

qwen_14b_q6:
  model_path: "C:\\llama\\models\\Qwen_Qwen3-14B-Q6_K.gguf"
  parameters:
    host: "127.0.0.1"
    port: 8080
    n_gpu_layers: 99
    max_context_length: 40960
    max_gen_length: 32768
    format: "jinja"
    reasoning_format: "deepseek"
    split_mode: "row"
    temp: 0.6
    top_k: 20
    top_p: 0.95
    min_p: 0
    no_context_shift: true
    enable_flash_attn: true

qwen_30b_a3b_q4:
  model_path: "C:\\llama\\models\\Qwen3-4B-UD-Q6_K_XL.gguf"
  parameters:
    host: "127.0.0.1"
    port: 8080
    n_gpu_layers: 99
    max_context_length: 40960
    max_gen_length: 32768
    format: "jinja"
    reasoning_format: "deepseek"
    split_mode: "row"
    temp: 0.6
    top_k: 20
    top_p: 0.95
    min_p: 0
    no_context_shift: true
    enable_flash_attn: true

phi_4_q6:
  model_path: "C:\\llama\\models\\phi-4-Q6_K_L.gguf"
  parameters:
    host: "127.0.0.1"
    port: 8080
    n_gpu_layers: 99
    max_context_length: 32768
    max_gen_length: 32768

mistral_24b_instruct_q5:
  model_path: "C:\\llama\\models\\Mistral-Small-3.1-24B-Instruct-2503-UD-Q5_K_XL.gguf"
  parameters:
    host: "127.0.0.1"
    port: 8080
    n_gpu_layers: 99
    max_context_length: 32768
    max_gen_length: 32768
    temp: 0.15
